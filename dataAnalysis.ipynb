{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import functions as f\n",
    "import classes as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "def testDTClassifier(training_inputs, training_targets, estimators=50, components=11, depth=5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(training_inputs, training_targets, test_size=0.7)\n",
    "    # Create an instance of the BoostedGaussianNaiveBayesClassifier\n",
    "    boosted_dt_classifier = c.BoostedDecisionTreeClassifier(n_estimators=estimators, n_components=components, max_depth=depth)\n",
    "\n",
    "    # Fit the classifier on the training data\n",
    "    boosted_dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    predicted_labels = boosted_dt_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    accuracy *= 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = f.getDataFrame(\"TrainOnMe.csv\")\n",
    "test_cleared_dataframe = f.clearData(test_df)\n",
    "test_training_inputs, test_training_targets = f.extractData(test_cleared_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER_NUM = 40\n",
    "ESTIMATOR_NUM = range(1, 100)\n",
    "\n",
    "# Save the errors after checking each of them through cross validation\n",
    "estimator_errors = []\n",
    "\n",
    "# Randomly select the training and testing data and iterate through it\n",
    "# TODO: implement the iteration and saving the errors for each parameter measured\n",
    "\n",
    "for n in ESTIMATOR_NUM:\n",
    "    iteration_error = []\n",
    "    for i in range(ITER_NUM):\n",
    "        iteration_error.append(1- (testDTClassifier(test_training_inputs, test_training_targets, estimators=n))/100.0 )\n",
    "    estimator_errors.append(iteration_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ESTIMATOR_NUM\n",
    "y, e = f.calculatePlotValues(estimator_errors)\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "\n",
    "plt.errorbar(x, y, yerr=e, linestyle='solid', marker='.', label='Error')\n",
    "plt.ylabel('Error fraction')\n",
    "plt.xlabel(\"Estimators\")\n",
    "plt.title(f\"\"\"Boosted Decision Tree error fraction based on the number of estimators\"\"\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER_NUM = 40\n",
    "DEPTH_NUM = range(1, 20)\n",
    "\n",
    "# Save the errors after checking each of them through cross validation\n",
    "depth_errors = []\n",
    "\n",
    "# Randomly select the training and testing data and iterate through it\n",
    "# TODO: implement the iteration and saving the errors for each parameter measured\n",
    "\n",
    "for n in DEPTH_NUM:\n",
    "    iteration_error = []\n",
    "    for i in range(ITER_NUM):\n",
    "        iteration_error.append(1- (testDTClassifier(test_training_inputs, test_training_targets, depth=n))/100.0 )\n",
    "    depth_errors.append(iteration_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DEPTH_NUM\n",
    "y, e = f.calculatePlotValues(depth_errors)\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "\n",
    "plt.errorbar(x, y, yerr=e, linestyle='solid', marker='.', label='Error')\n",
    "plt.ylabel('Error fraction')\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.title(f\"\"\"Boosted Decision Tree error fraction based on the depth\"\"\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER_NUM = 80\n",
    "COMP_NUM = range(1, 14)\n",
    "\n",
    "# Save the errors after checking each of them through cross validation\n",
    "comp_errors = []\n",
    "\n",
    "# Randomly select the training and testing data and iterate through it\n",
    "# TODO: implement the iteration and saving the errors for each parameter measured\n",
    "\n",
    "for n in COMP_NUM:\n",
    "    iteration_error = []\n",
    "    for i in range(ITER_NUM):\n",
    "        iteration_error.append(1- (testDTClassifier(test_training_inputs, test_training_targets, components=n, estimators=80, depth=5))/100.0 )\n",
    "    comp_errors.append(iteration_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = COMP_NUM\n",
    "y, e = f.calculatePlotValues(comp_errors)\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "\n",
    "plt.errorbar(x, y, yerr=e, linestyle='solid', marker='.', label='Error')\n",
    "plt.ylabel('Error fraction')\n",
    "plt.xlabel(\"Components\")\n",
    "plt.title(f\"\"\"Boosted Decision Tree error fraction based on the components\"\"\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6feb98f127bf4ece1f9cfb2be77a6e249974be563538449934d65557d42256ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
